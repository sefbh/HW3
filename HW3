---
title: "HW 3"
author: "Student Name"
date: "10/6/2024"
output: 
  html_document:
    number_sections: true
---

#

Let $E[X] = \mu$.  Show that $Var[X] := E[(X-E[X])^2] = E[X^2]-(E[X])^2$.  Note, all you have to do is show the second equality (the first is our definition from class). 

Var(X)=E[(Xâˆ’E[X])^2]

# 

In the computational section of this homework, we will discuss support vector machines and tree-based methods.  I will begin by simulating some data for you to use with SVM. 

```{r}
library(e1071)
set.seed(1) 
x=matrix(rnorm(200*2),ncol=2)
x[1:100,]=x[1:100,]+2
x[101:150,]=x[101:150,]-2
y=c(rep(1,150),rep(2,50))
dat=data.frame(x=x,y=as.factor(y))
plot(x, col=y)

```


##

Quite clearly, the above data is not linearly separable.  Create a training-testing partition with 100 random observations in the training partition.  Fit an svm on this training data using the radial kernel, and tuning parameters $\gamma=1$, cost $=1$.  Plot the svm on the training data.  

```{r}
library(e1071)
set.seed(1) 
x=matrix(rnorm(200*2),ncol=2)
x[1:100,]=x[1:100,]+2
x[101:150,]=x[101:150,]-2
y=c(rep(1,150),rep(2,50))
dat=data.frame(x=x,y=as.factor(y))
plot(x, col=y)
train_index <- sample(1:200, 100)
train_data <- dat[train_index, ]
test_data <- dat[-train_index, ]
svmfit <- svm(y ~., data = train_data, kernel = "radial", gamma = 1, cost = 1)
plot(svmfit, train_data)
```

##

Notice that the above decision boundary is decidedly non-linear.  It seems to perform reasonably well, but there are indeed some misclassifications.  Let's see if increasing the cost ^[Remember this is a parameter that decides how smooth your decision boundary should be] helps our classification error rate.  Refit the svm with the radial kernel, $\gamma=1$, and a cost of 10000.  Plot this svm on the training data. 

```{r}
svmfit_high_cost <- svm(y ~., data = train_data, kernel = "radial", gamma = 1, cost = 10000)
plot(svmfit_high_cost, train_data)

```

##

It would appear that we are better capturing the training data, but comment on the dangers (if any exist), of such a model. 

*Such a model can result in overfitting. Just because it captures the training data better it might not capture other data because it would be too sensitive and less generalizable.*

##

Create a confusion matrix by using this svm to predict on the current testing partition.  Comment on the confusion matrix.  Is there any disparity in our classification results?    

```{r}
#remove eval = FALSE in above
table(true=dat[-train, "y"], pred=predict(svmfit, newdata=dat[-train,]))
# Predict on the testing data
pred <- predict(svmfit_high_cost, newdata=test_data)
confusion_matrix <- table(True=test_data$y, Pred=pred)
#Print out the matrix
print(confusion_matrix)

```


##

Is this disparity because of imbalance in the training/testing partition?  Find the proportion of class `2` in your training partition and see if it is broadly representative of the underlying 25\% of class 2 in the data as a whole.  

```{r}
# checking the props
prop_class2_train <- sum(train_data$y == 2) / length(train_data$y)
prop_class2_total <- sum(dat$y == 2) / length(dat$y)
#printing the props
prop_class2_train
prop_class2_total


```

*The disparity is because of imbalance in the training/testing partition since class 2 is underrepresented in the training set. If class 2 is underrepresented, the support vecto machime might struggle make a pattern for class 2 and cause misclassifications.*

##

Let's try and balance the above to solutions via cross-validation.  Using the `tune` function, pass in the training data, and a list of the following cost and $\gamma$ values: {0.1, 1, 10, 100, 1000} and {0.5, 1,2,3,4}.  Save the output of this function in a variable called `tune.out`.  

```{r}
set.seed(1)
tune.out <- tune(svm, y ~., data = train_data, kernel = "radial", ranges=list(cost=c(0.1, 1, 10, 100, 1000), gamma=c(0.5, 1, 2, 3, 4)))
best_model <- tune.out$best.model
pred_best <- predict(best_model, newdata=test_data)
# confusion matrix
confusion_matrix_best <- table(True=test_data$y, Pred=pred_best)
print(confusion_matrix_best)


```

I will take `tune.out` and use the best model according to error rate to test on our data.  I will report a confusion matrix corresponding to the 100 predictions.  


```{r, eval = FALSE}
table(true=dat[-train,"y"], pred=predict(tune.out$best.model, newdata=dat[-train,]))
```

##

Comment on the confusion matrix.  How have we improved upon the model in question 2 and what qualifications are still necessary for this improved model.  

*The new confusion matrix shows fewer errors and is more generalized to avoid overfitting. It's also better at classifying both classes because tuning out the matrix created a better balance between margin size and classification error. However, there are still some improvements to be made. Since class 2 is still underrepresented in the training data, the confusion matrix might still show more misclassifications for class 2. Cross-validation didn't address this class imbalance, so the tuned model doesn't do anything against this.*

# 
Let's turn now to decision trees.  

```{r}

library(kmed)
data(heart)
library(tree)

```

## 

The response variable is currently a categorical variable with four levels.  Convert heart disease into binary categorical variable.  Then, ensure that it is properly stored as a factor. 

```{r}
heart_disease = ifelse(heart$disease == "No", 0, 1)
heart_disease = as.factor(heart$disease)

```

## 

Train a classification tree on a 240 observation training subset (using the seed I have set for you).  Plot the tree.  

```{r}
library(kmed)
data(heart)
library(tree)
set.seed(101)
library(rpart.plot)
heart.tree <- rpart(heart_disease~., data = train_data, method = "class")
par(xpd = NA)
plot(heart.tree)
text(heart.tree, pretty = 0)
# I can't figure out what to put where heart is in line 162, nothing works
```


## 

Use the trained model to classify the remaining testing points.  Create a confusion matrix to evaluate performance.  Report the classification error rate.  

```{r}
pred_tree <- predict(heart.tree, newdata=test_data, type="class")
# create matrix
confusion_matrix_tree <- table(True=test_data$binary_disease, Pred=pred_tree)
# print matrix
print(confusion_matrix_tree)

```

##  

Above we have a fully grown (bushy) tree.  Now, cross validate it using the `cv.tree` command.  Specify cross validation to be done according to the misclassification rate.  Choose an ideal number of splits, and plot this tree.  Finally, use this pruned tree to test on the testing set.  Report a confusion matrix and the misclassification rate.  

```{r}
set.seed(101)
cv_tree = cv.tree(heart.tree, FUN = prune.misclass)
pruned_tree = prune.misclass(tree_model, best=cv_tree$size[which.min(cv_tree$dev)])
plot(pruned_tree)
text(pruned_tree, pretty=0)
pred_pruned = predict(pruned_tree, heart[-train,], type="class")
# make confusion matrix
confusion_matrix_pruned <- table(True=test_data$heart_disease, Pred=pred_pruned)
# print confusion matrix
print(confusion_matrix_pruned)
```


##

Discuss the trade-off in accuracy and interpretability in pruning the above tree. 

*Pruning a tree can improve the interpretability of data by focusing only on the more important variables, and simply by making the graphic easier to follow. Pruned trees are less complex and can be more generalized. However, getting rid of some of the end variables can reduce the accuracy because it reduces some of the variable splits. This can result in underfitting.*

## 

Discuss the ways a decision tree could manifest algorithmic bias.  

*Algorithmic bias can occur in decision trees when the training data is unbalanced. It can also happen when variables disproportionally influence the splits. Additionally, if the tree is overfit, then it isn't generalizable. You need representative training data to avoid bias.*
